{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import torch\n",
    "from torchtext import datasets\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "# contains full long passages from wikipedia\n",
    "corpus = datasets.WikiText2(split='train')\n",
    "\n",
    "# create one big slice of text \n",
    "corpus = ''.join(list(islice(corpus, 10000)))\n",
    "\n",
    "# divide into words\n",
    "corpus_words = corpus.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596873\n",
      "23762\n",
      "23619\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "vocab = nltk.word_tokenize(corpus)\n",
    "print(len(vocab))\n",
    "vocab = list(set(vocab))\n",
    "print(len(vocab))\n",
    "# nltk.download('stopwords')\n",
    "vocab = [i for i in vocab if i not in nltk.corpus.stopwords.words('english')]\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping between words to numbers\n",
    "vocab_to_int = {word: i+1 for i, word in enumerate(vocab)}\n",
    "vocab_to_int[\"UNK\"] = 0\n",
    "vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two vectors of parameters\n",
    "center = torch.rand(size=(len(vocab), 1))\n",
    "context = torch.rand(size=(len(vocab), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.3772]), tensor([0.9885]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center[vocab_to_int['people']], context[vocab_to_int['people']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(“'The|people)\n",
      "P(common|people)\n",
      "P(pray|people)\n",
      "P(for|people)\n",
      "\n",
      "P(common|pray)\n",
      "P(people|pray)\n",
      "P(for|pray)\n",
      "P(rain,|pray)\n",
      "\n",
      "P(people|for)\n",
      "P(pray|for)\n",
      "P(rain,|for)\n",
      "P(healthy|for)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = ''' “'The common people pray for rain, healthy children, and a summer that never ends', Ser Jorah told her. 'It is no matter to them if the high lords play their game of thrones, so long as they are left in peace.' He gave a shrug. 'They never are.'” '''.split()\n",
    "\n",
    "context_window = 2\n",
    "\n",
    "for i in range(context_window, len(text[:7])-context_window):\n",
    "    center_word = text[i]\n",
    "\n",
    "\n",
    "    for j in range(i-context_window, i+context_window+1):\n",
    "        if j == i: \n",
    "            continue\n",
    "        context_word = text[j]\n",
    "        print(f\"P({context_word}|{center_word})\", )\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
